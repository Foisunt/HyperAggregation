{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a23b1e11-69ad-4ebf-80ad-397748eb6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Actor, WikipediaNetwork, HeterophilousGraphDataset\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c1aa9e-db9f-490b-b9bd-e32a8c25494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create own (pitfalls/geom gcn) split (per class 20 train, 30 val, others test)\n",
    "def create_23_split(ds_name):\n",
    "    #dataset = Amazon(root='dataset/'+ds_name+\"/\", name=ds_name)\n",
    "    dataset = Planetoid(root='dataset/'+ds_name+\"/\", name=ds_name)\n",
    "    \n",
    "    y = dataset[0].y.cpu().detach().numpy()\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    train_mask = []\n",
    "    val_mask = []\n",
    "    test_mask = []\n",
    "    for seed in tqdm(range(10)):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        train = []\n",
    "        val = []\n",
    "        test = []\n",
    "\n",
    "        for cl in unique:\n",
    "            tmp = np.argwhere(y==cl)\n",
    "            rng.shuffle(tmp)\n",
    "            train.append(tmp[:20])\n",
    "            val.append(tmp[20:50])\n",
    "            test.append(tmp[50:])\n",
    "\n",
    "        train_ix = np.concatenate(train)\n",
    "        val_ix = np.concatenate(val)\n",
    "        test_ix = np.concatenate(test)\n",
    "\n",
    "        train = torch.full_like(dataset[0].y, False, dtype=torch.bool)\n",
    "        train[train_ix] = True\n",
    "        val = torch.full_like(dataset[0].y, False, dtype=torch.bool)\n",
    "        val[val_ix] = True\n",
    "        test = torch.full_like(dataset[0].y, False, dtype=torch.bool)\n",
    "        test[test_ix] = True\n",
    "        train_mask.append(train)\n",
    "        val_mask.append(val)\n",
    "        test_mask.append(test)\n",
    "    dict = {\"train\":torch.stack(train_mask, 1), \"valid\":torch.stack(val_mask, 1), \"test\":torch.stack(test_mask, 1)}\n",
    "    torch.save(dict,\"dataset/\"+ds_name+\"/own_23_splits.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "528e89d5-f729-4db4-b855-ffc4a05516a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 110.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 69.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# for name in [\"Photo\", \"Computers\"]:\n",
    "#     create_23_split(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6811a0-0b65-45ce-97b5-10c4d5412f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 372.95it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 259.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 51.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# for name in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n",
    "#     create_23_split(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0e702e-980d-43f4-803f-2ce862e4dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(name):\n",
    "        splits = None\n",
    "        if name in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n",
    "            ds = Planetoid(root='dataset/'+name+\"/\", name=name)\n",
    "            splits = torch.load(\"dataset/\"+name+\"/own_23_splits.pt\")\n",
    "        elif name in [\"Roman-empire\", \"Minesweeper\"]:\n",
    "            ds = HeterophilousGraphDataset(root='dataset/'+name+\"/\", name=name)\n",
    "            splits = {\"train\":ds[0].train_mask, \"valid\":ds[0].val_mask, \"test\":ds[0].test_mask}\n",
    "        elif name in [\"Photo\", \"Computers\"]:\n",
    "            ds = Amazon(root='dataset/'+name+\"/\", name=name)\n",
    "            splits = torch.load(\"dataset/\"+name+\"/own_23_splits.pt\")\n",
    "        elif name in [\"Chameleon\", \"Squirrel\"]:\n",
    "            ds = WikipediaNetwork(root=\"dataset/\"+name+\"/\", name = name)\n",
    "            splits = {\"train\":ds[0].train_mask, \"valid\":ds[0].val_mask, \"test\":ds[0].test_mask}\n",
    "        elif name == \"Actor\":\n",
    "            ds = Actor(root=\"dataset/Actor/\")\n",
    "            splits = {\"train\":ds[0].train_mask, \"valid\":ds[0].val_mask, \"test\":ds[0].test_mask}\n",
    "        elif name == \"Arxiv\":\n",
    "            ds = PygNodePropPredDataset(name = \"ogbn-arxiv\")\n",
    "            splits = ds.get_idx_split()\n",
    "        elif name == \"Products\":\n",
    "            ds = PygNodePropPredDataset(name = \"ogbn-products\")\n",
    "            splits = ds.get_idx_split()\n",
    "        return ds, splits\n",
    "\n",
    "#create own NOSMOG inductive split (80% of test as unlabeled structure)\n",
    "def create_ind82_split(ds_name):\n",
    "    dataset, split = load_ds(ds_name)\n",
    "\n",
    "    if ds_name not in [\"Arxiv\", \"Products\"]: #masks are boolean and provided per split\n",
    "        struct_msk = []\n",
    "        test_msk = []\n",
    "        for seed in range(10):\n",
    "            rng = np.random.default_rng(seed)\n",
    "            tmp = np.argwhere(split[\"test\"][:,seed]==1)[0].numpy()\n",
    "            rng.shuffle(tmp)\n",
    "            l = len(tmp)\n",
    "            struc = torch.full_like(dataset[0].y, False, dtype=torch.bool)\n",
    "            struc[tmp[0:round(0.8*l)]]=True\n",
    "            test = torch.full_like(dataset[0].y, False, dtype=torch.bool)\n",
    "            test[tmp[round(0.8*l):]] = True\n",
    "            struct_msk.append(struc)\n",
    "            test_msk.append(test)\n",
    "    elif ds_name in [\"Arxiv\", \"Products\"]: #fixed train test splits and masks are index lists\n",
    "        struct_msk = []\n",
    "        test_msk = []\n",
    "        for seed in range(10):\n",
    "            rng = np.random.default_rng(seed)\n",
    "            tmp = split[\"test\"].numpy()\n",
    "            rng.shuffle(tmp)\n",
    "            l = len(tmp)\n",
    "            struct_msk.append(torch.from_numpy(tmp[0:round(0.8*l)]))\n",
    "            test_msk.append(torch.from_numpy(tmp[round(0.8*l):]))\n",
    "        ds_name = \"ogbn_\" + ds_name.lower()\n",
    "    else:\n",
    "        raise NotImplementedError(ds_name+ \" is an unkown ds\")\n",
    "\n",
    "    dict = {\"train\":split[\"train\"], \"valid\":split[\"valid\"], \"structure\":torch.stack(struct_msk, 1), \"test\":torch.stack(test_msk, 1)}\n",
    "    torch.save(dict,\"dataset/\"+ds_name+\"/own_82_splits.pt\")\n",
    "    #print(ds_name, split[\"test\"].shape, dict[\"structure\"].shape, dict[\"test\"].shape, split[\"test\"].sum(), dict[\"structure\"].sum(), dict[\"test\"].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caab5489-ec7d-4e22-ba26-67cac1e7571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [\"Cora\", \"CiteSeer\", \"PubMed\", \"Computers\", \"Photo\", \"Chameleon\", \"Squirrel\", \"Actor\",\"Roman-empire\", \"Minesweeper\"]:#, \"Arxiv\"]:\n",
    "    create_ind82_split(ds)\n",
    "    #d, s = load_ds(ds)\n",
    "    #print(ds, s[\"train\"].shape, s[\"test\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3777043d-b31c-40b8-9518-696b07a49658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 46.18it/s]\n"
     ]
    }
   ],
   "source": [
    "#create_23_split(\"Computers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40dcaca0-594f-48f9-92f8-b8fe0e561614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 72.54it/s]\n"
     ]
    }
   ],
   "source": [
    "#create_23_split(\"Photo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86919d36-d977-42c4-8d39-769358970ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
